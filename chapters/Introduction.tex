\chapter{Introduction}
\label{cha:introduction}

\section{Research Background and Motivation}
\label{sec:introduction:background_motivation}

With the advent of Industry 4.0, modern industrial systems are becoming increasingly complex and automated \cite{chen2017industry}. In this context, ensuring the reliability and safety of machinery is of paramount importance. Unexpected equipment failures can lead to significant economic losses, production disruptions, and even safety accidents. Therefore, intelligent fault diagnosis technology, which can detect and identify potential faults in a timely and accurate manner, has become a key research area in industrial applications \cite{zhang2019deep, zhao2019deep}. Traditional fault diagnosis methods, which often rely on physical models or signal processing techniques combined with expert knowledge, struggle to cope with the massive, high-dimensional, and complex time-series data generated by modern sensor networks \cite{lei2016intelligent}. The data collected from industrial processes, such as vibration, pressure, and temperature signals, are often characterized by strong non-linearity and non-stationarity, making it difficult for these conventional methods to effectively extract discriminative fault features.

In recent years, data-driven approaches, particularly those based on deep learning, have shown great potential in overcoming these limitations \cite{liu2018artificial}. Deep learning models can automatically learn hierarchical features directly from raw sensor data, circumventing the need for laborious and often subjective manual feature engineering \cite{khan2018review}. Among various deep learning architectures, Long Short-Term Memory (LSTM) networks, a specialized type of recurrent neural network (RNN), are particularly well-suited for modeling time-series data due to their ability to capture temporal dependencies \cite{hochreiter1997long}. LSTMs have been successfully applied to fault detection in industrial time series, demonstrating their effectiveness in learning from sequential patterns \cite{filonov2016multivariateindustrialtimeseries}. 

However, LSTMs process information sequentially, which can lead to challenges in capturing long-range dependencies in very long sequences and may suffer from computational inefficiency. More recently, the Transformer model, which relies entirely on a self-attention mechanism, has achieved state-of-the-art performance in various sequence modeling tasks \cite{vaswani2017attention}. The self-attention mechanism allows the Transformer to weigh the importance of all time steps simultaneously, enabling it to capture global dependencies and interactions within a sequence. This capability offers a new and powerful perspective for time-series analysis and has shown promise in fault diagnosis \cite{wen2021time}. Nevertheless, the standard Transformer architecture may not sufficiently capture the fine-grained, ordered nature of fault signatures as effectively as recurrent models.

This reveals a clear research gap: LSTMs excel at modeling local, sequential patterns but may miss critical long-range dependencies, while Transformers are adept at capturing global context but may overlook the nuanced, ordered progression inherent in fault evolution. This thesis is motivated by the potential of synergistically combining the complementary strengths of both architectures. We hypothesize that a hybrid model can achieve superior performance by leveraging the Transformer\'s ability to extract globally relevant features and then feeding these context-rich representations into an LSTM to model the temporal evolution of the system\'s health state. Furthermore, this research aims to address practical challenges such as class imbalance and multi-scale temporal patterns by proposing a novel hybrid Transformer-LSTM model that incorporates an adaptive fusion mechanism and a focal loss function, creating a comprehensive and robust fault diagnosis system.

\section{Analysis of Current Research Status}
\label{sec:introduction:current_status}

The study of fault diagnosis technology in the industrial sector can be broadly categorized into three phases: physics-based models, traditional machine learning-based methods, and deep learning-based approaches.

Early research primarily focused on physics-based models and signal processing techniques. These methods attempt to build precise mathematical models for specific equipment or utilize techniques like Fourier Transform and Wavelet Transform to analyze signal characteristics in the time or frequency domain. However, these approaches often require extensive prior knowledge, involve complex model construction, and have limited generalization capabilities when faced with complex and variable operating conditions. Subsequently, traditional machine learning methods, represented by Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), and Artificial Neural Networks (ANN), were introduced. While these methods improved the level of automation in diagnostics, their performance heavily relied on manually engineered features. This process is not only time-consuming and labor-intensive but also requires deep domain expertise, making it difficult to achieve end-to-end intelligent diagnosis \cite{lei2020applications}.

In recent years, data-driven methods, particularly those based on deep learning, have become the mainstream research direction in intelligent fault diagnosis due to their powerful capabilities in automatic feature extraction and non-linear modeling \cite{zhao2019deep, zhang2019deep}. Various deep learning models have been successfully applied:

\begin{itemize}
    \item \textbf{Convolutional Neural Networks (CNN):} Initially achieving great success in image recognition, CNNs are now widely used to process one-dimensional signals like vibration and acoustic data, or their two-dimensional time-frequency representations. Through their unique convolutional and pooling operations, CNNs can effectively capture local correlations and translation-invariant features in signals, making them highly suitable for extracting discriminative fault patterns \cite{liu2018artificial}.
    \item \textbf{Recurrent Neural Networks (RNN) and their variants:} RNNs, especially Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, are specifically designed for processing sequential data \cite{hochreiter1997long}. They can effectively capture temporal dependencies, which is crucial for understanding the entire lifecycle of a fault from inception to evolution \cite{filonov2016multivariateindustrialtimeseries}. Nevertheless, as mentioned earlier, these models may struggle to capture long-range dependencies in very long sequences due to issues like vanishing gradients.
    \item \textbf{The Transformer Model:} To address the problem of long-range dependencies, the Transformer model, originally developed for natural language processing, was introduced to time-series analysis \cite{vaswani2017attention}. Its core self-attention mechanism allows for the parallel computation of dependencies between all elements in a sequence, enabling the direct capture of global information. This has provided a new perspective for fault diagnosis, and studies have shown the great potential of Transformer-based models in time-series forecasting and classification tasks \cite{zhou2021informer}.
\end{itemize}

To combine the advantages of different models for superior performance, researchers have begun to explore hybrid deep learning models. For instance, using a CNN to extract spatial features and then feeding the output to an LSTM for time-series modeling (CNN-LSTM) is a common and proven strategy \cite{khan2018review}. The success of such combined models inspires us that further improvements in diagnostic accuracy and robustness can be achieved by integrating the structural strengths of different models. However, the deep fusion of the Transformer\'s powerful global context-aware capabilities with the LSTM\'s fine-grained sequential evolution modeling is still in its nascent stages. This provides a clear entry point and a promising innovative direction for the research in this thesis.



\section{Core Issues and Challenges}
\label{sec:introduction:issues_challenges}

Despite the progress in deep learning-based fault diagnosis, several core issues and challenges remain to be addressed to enhance the performance, robustness, and applicability of these models in real-world industrial settings. This thesis will focus on the following key challenges:

\subsection{Modeling of Long-Term Time-Dependent Relationships}
Industrial processes often exhibit complex temporal dynamics where the signature of a fault may evolve over a long period. Capturing these long-range dependencies is crucial for early and accurate diagnosis. While LSTMs are designed to model temporal sequences, they can struggle with very long sequences due to the risk of vanishing or exploding gradients \cite{pascanu2013difficulty}, making it difficult to relate current observations to events far in the past. The challenge lies in developing a model that can effectively bridge these long time lags and understand the complete temporal evolution of system behavior.

\subsection{Data Class Imbalance}
In real-world industrial applications, datasets for fault diagnosis are often highly imbalanced. Data for normal operating conditions are abundant, while data for specific fault types, especially critical or catastrophic failures, are scarce. This imbalance can severely bias a machine learning model, causing it to perform well on the majority class (normal operation) but poorly on the minority classes (faults). The challenge is to design a model and training strategy that can learn effectively from imbalanced data, for instance by using specialized sampling techniques \cite{chawla2002smote}, and accurately identify rare but critical fault events.

\subsection{Multi-Scale Time Pattern Analysis}
Fault signatures in time-series data can manifest at multiple time scales. For example, a gradual degradation process might be a long-term trend, while an intermittent fault could appear as a short-term, high-frequency burst. A robust diagnostic system must be capable of simultaneously analyzing and integrating information from these different time scales. Traditional models often focus on a single temporal resolution, potentially missing important patterns at other scales. The challenge is to develop an architecture that can effectively extract and fuse multi-scale temporal features \cite{sun2022multiscale} to form a comprehensive understanding of the system\'s health state.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{logos/multi_scale_timeseries_decomposition.pdf}
\caption{An illustration of a complex time-series signal decomposed into its multi-scale components: a long-term trend, periodic patterns, and high-frequency noise.}
\label{fig:multi_scale_decomposition}
\end{figure}

\subsection{Generalization Ability to Different Working Conditions}
Industrial equipment often operates under various working conditions, such as different loads, speeds, or environmental factors. A fault diagnosis model trained on data from one set of conditions may not generalize well to others, as the fault signatures can vary significantly. This lack of generalization, often addressed with transfer learning techniques \cite{wen2021review}, limits the practical deployment of diagnostic systems. The challenge is to build a model that is robust to variations in operating conditions and can extract invariant fault features, ensuring reliable performance across the full operational spectrum of the machinery.

\section{Main Research Content and Contributions}
\label{sec:introduction:content_contributions}

To address the aforementioned challenges in industrial fault diagnosis, this thesis systematically designs, implements, and validates a novel hybrid deep learning system. The primary research content of this work is centered around the development of a hybrid architecture that integrates the Transformer and Long Short-Term Memory (LSTM) networks to achieve superior diagnostic performance. The key tasks undertaken in this research include:

Designing a Hybrid Transformer-LSTM Architecture: The core of this work is to construct a model that capitalizes on the complementary strengths of its components. A Transformer encoder is employed to capture global dependencies and long-range correlations from multivariate sensor data, while a subsequent bidirectional LSTM (Bi-LSTM) network models the temporal evolution and sequential nature of the extracted features.

Developing an Adaptive Feature Fusion Strategy: A crucial aspect of the hybrid model is the effective integration of features from the Transformer and LSTM components. This research proposes and implements an adaptive fusion mechanism that dynamically weighs the contributions of global contextual features and local temporal patterns, allowing the model to focus on the most salient information for a given diagnostic task.

Implementing Solutions for Real-World Challenges: The study goes beyond the core architecture to tackle practical issues prevalent in industrial settings. This includes utilizing a focal loss function to mitigate the effects of class imbalance, applying targeted data augmentation techniques for time-series data, and ensuring the model\'s robustness through appropriate regularization and training strategies.

Conducting Comprehensive Experimental Validation: The proposed model is rigorously evaluated on a real-world industrial dataset containing multiple fault types (e.g., backlash, baseline shift, jerk). Its performance is systematically compared against baseline models, including standalone Transformer and LSTM architectures, through extensive experiments and ablation studies.

The main contributions of this thesis, both theoretical and practical, are summarized as follows:

A Novel Hybrid Deep Learning Architecture for Fault Diagnosis: This thesis introduces a new hybrid model that effectively combines the Transformer and LSTM networks. This architecture provides a new paradigm for industrial time-series analysis by concurrently modeling global feature interactions and local temporal dynamics, addressing a key limitation of using either model in isolation \cite{wen2021time}.

An Adaptive Fusion Mechanism for Enhanced Feature Representation: We propose an adaptive fusion strategy that intelligently integrates multi-level features. Unlike simple concatenation, this mechanism allows the model to learn the optimal balance between global and sequential representations, leading to a more robust and discriminative feature space for classification.

A Comprehensive Solution to Common Industrial Data Challenges: This research presents a holistic approach that simultaneously addresses multiple practical problems. By integrating techniques like focal loss for class imbalance \cite{lin2017focal} and advanced regularization, this work provides a more complete and resilient solution tailored for real-world industrial deployment.

Empirical Validation and a Practical Blueprint for Industrial Application: Through extensive experimentation, this thesis validates the superiority of the proposed model over conventional and single-architecture deep learning methods. The detailed methodology and positive results serve as a practical blueprint for deploying advanced AI-driven predictive maintenance systems in industrial environments.

\section{Thesis Chapter Organization}
\label{sec:introduction:organization}

The remainder of this thesis is structured as follows:

Chapter 2: Related Technologies and Theoretical Foundations provides a comprehensive review of the fundamental concepts underpinning this research. It covers the principles of Long Short-Term Memory (LSTM) networks, the architecture of the Transformer model, including the self-attention mechanism, and other key deep learning techniques relevant to this study.

Chapter 3: Fault Diagnosis Model based on Hybrid Transformer-LSTM details the proposed model. It presents the overall architecture, describes the data preprocessing pipeline, and elaborates on the design of the Transformer feature extraction component, the LSTM sequence modeling component, and the core adaptive feature fusion strategy.

Chapter 4: Experiments and Result Analysis describes the experimental setup, including the dataset, evaluation metrics, and hyperparameter settings. It presents a detailed analysis of the experimental results, including performance comparisons with baseline models, ablation studies to validate the contribution of each model component, and visualizations such as confusion matrices and learning curves.

Chapter 5: Conclusion and Future Work summarizes the key findings and and contributions of this research. It also discusses the limitations of the current work and suggests potential directions for future research, such as model optimization for real-time deployment and exploration of unsupervised learning methods.