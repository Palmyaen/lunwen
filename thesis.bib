% [1] Industry 4.0 survey
@article{chen2017industry,
  title={Industry 4.0: A survey on technologies, applications and open research issues},
  author={Lu, Yang},
  journal={Journal of Industrial Information Integration},
  volume={6},
  pages={1--10},
  year={2017},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.jii.2017.04.005}
}

% [2] Deep residual learning fault diagnosis
@article{zhang2019deep,
  title={Deep residual learning-based fault diagnosis method for rotating machinery},
  author={Zhang, Wei and Li, Chuanhao and Peng, Gaoliang and Chen, Yuanhang and Zhang, Zhujun},
  journal={ISA transactions},
  volume={95},
  pages={295--305},
  year={2019},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.isatra.2018.12.025}
}

% [3] Deep learning machine health monitoring
@article{zhao2019deep,
  title={Deep learning and its applications to machine health monitoring},
  author={Zhao, Rui and Yan, Ruqiang and Chen, Zhenghua and Mao, Kezhi and Wang, Peng and Gao, Robert X},
  journal={Mechanical Systems and Signal Processing},
  volume={115},
  pages={213--237},
  year={2019},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.ymssp.2018.05.050}
}

% [4] Machinery health prognostics review
@article{lei2016intelligent,
  title={Machinery health prognostics: A systematic review from data acquisition to RUL prediction},
  author={Lei, Yaguo and Li, Naipeng and Guo, Liang and Li, Nilanjan and Yan, Tao and Lin, Jing},
  journal={Mechanical Systems and Signal Processing},
  volume={104},
  pages={799--834},
  year={2018},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.ymssp.2017.11.016}
}

% [5] AI fault diagnosis review
@article{liu2018artificial,
  title={Artificial intelligence for fault diagnosis of rotating machinery: A review},
  author={Liu, Ruonan and Yang, Boyuan and Zio, Enrico and Chen, Xuefeng},
  journal={Mechanical Systems and Signal Processing},
  volume={108},
  pages={33--47},
  year={2018},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.ymssp.2018.02.016}
}

% [6] Deep learning system health management
@article{khan2018review,
  title={A review on the application of deep learning in system health management},
  author={Khan, Samir and Yairi, Takehisa},
  journal={Mechanical Systems and Signal Processing},
  volume={107},
  pages={241--265},
  year={2018},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.ymssp.2017.11.024}
}

% [7] LSTM original paper
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  url={https://doi.org/10.1162/neco.1997.9.8.1735}
}

% [8] Multivariate industrial time series LSTM
@article{filonov2016multivariateindustrialtimeseries,
  title={Multivariate industrial time series with cyber-attack simulation: Fault detection using an LSTM-based predictive data model},
  author={Filonov, Pavel and Lavrentyev, Andrey and Vorontsov, Artem},
  journal={arXiv preprint arXiv:1612.06676},
  year={2016},
  url={https://arxiv.org/abs/1612.06676}
}


@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

% [10] Time series data augmentation survey
@article{wen2021time,
  title={Time series data augmentation for deep learning: a survey},
  author={Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin and Gao, Jingkun and Wang, Xue and Xu, Huan},
  journal={arXiv preprint arXiv:2002.12478},
  year={2020},
  url={https://arxiv.org/abs/2002.12478}
}

% [11] ML machine fault diagnosis review
@article{lei2020applications,
  title={Applications of machine learning to machine fault diagnosis: A review and roadmap},
  author={Lei, Yaguo and Yang, Biao and Jiang, Xinwei and Jia, Feng and Li, Naipeng and Nandi, Asoke K},
  journal={Mechanical Systems and Signal Processing},
  volume={138},
  pages={106587},
  year={2020},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.ymssp.2019.106587}
}

% [12] Informer time-series forecasting
@article{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021},
  url={https://arxiv.org/abs/2012.07436}
}

% [13] RNN training difficulty
@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR},
  url={https://arxiv.org/abs/1211.5063}
}

% [14] SMOTE oversampling technique
@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002},
  url={https://doi.org/10.1613/jair.953}
}

% [15] Focal loss for object detection
@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017},
  url={https://arxiv.org/abs/1708.02002}
}

% [16] Layer normalization
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016},
  url={https://arxiv.org/abs/1607.06450}
}

% [17] Neural machine translation attention
@article{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2015},
  url={https://arxiv.org/abs/1409.0473}
}

% [18] Self-attention relative position
@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018},
  url={https://arxiv.org/abs/1803.02155}
}

% [19] Linformer linear complexity attention
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020},
  url={https://arxiv.org/abs/2006.04768}
}

% [20] Performers attention mechanism
@article{choromanski2021rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020},
  url={https://arxiv.org/abs/2009.14794}
}

% [21] Adam optimizer
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014},
  url={https://arxiv.org/abs/1412.6980}
}

% [22] AdamW weight decay
@article{loshchilov2019decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017},
  url={https://arxiv.org/abs/1711.05101}
}

% [23] SGDR cosine annealing
@article{loshchilov2016sgdr,
  title={SGDR: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016},
  url={https://arxiv.org/abs/1608.03983}
}

% [24] Super-convergence training
@article{smith2018superconvergence,
  title={Super-convergence: very fast training of neural networks using large learning rates},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1708.07120},
  year={2017},
  url={https://arxiv.org/abs/1708.07120}
}

% [25] Dropout regularization
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitesh and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR.org},
  url={https://jmlr.org/papers/v15/srivastava14a.html}
}

% [26] Deep learning book
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press},
  url={https://www.deeplearningbook.org/}
}

% [27] Learning from imbalanced data
@article{he2009learning,
  title={Learning from imbalanced data},
  author={He, Haibo and Garcia, Edwardo A},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={21},
  number={9},
  pages={1263--1284},
  year={2009},
  publisher={IEEE},
  url={https://doi.org/10.1109/TKDE.2008.239}
}

% [28] Imbalanced data challenges
@article{krawczyk2016learning,
  title={Learning from imbalanced data: open challenges and future directions},
  author={Krawczyk, Bartosz},
  journal={Progress in Artificial Intelligence},
  volume={5},
  number={4},
  pages={221--232},
  year={2016},
  publisher={Springer},
  url={https://doi.org/10.1007/s13748-016-0094-0}
}

% [29] Precision-Recall vs ROC
@article{saito2015precision,
  title={The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets},
  author={Saito, Takaya and Rehmsmeier, Marc},
  journal={PloS one},
  volume={10},
  number={3},
  pages={e0118432},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA},
  url={https://doi.org/10.1371/journal.pone.0118432}
}

% [30] Statistical learning book
@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer Science \& Business Media},
  url={https://web.stanford.edu/~hastie/ElemStatLearn/}
}

% [31] Precision-Recall ROC relationship
@inproceedings{davis2006relationship,
  title={The relationship between Precision-Recall and ROC curves},
  author={Davis, Jesse and Goadrich, Mark},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={233--240},
  year={2006},
  organization={ACM},
  url={https://doi.org/10.1145/1143844.1143874}
}

% [32] RNN language model - seminal work
@inproceedings{mikolov2010recurrent,
  title={Recurrent Neural Network Based Language Model},
  author={Mikolov, Tom{\'a}s and Karafi{\'a}t, Martin and Burget, Luk{\'a}s and {\v C}ernock{\'y}, Jan and Khudanpur, Sanjeev},
  booktitle={Proceedings of the Eleventh Annual Conference of the International Speech Communication Association},
  pages={2877--2880},
  year={2010},
  address={Chiba, Japan},
  month={26--30 September},
  organization={ISCA},
  url={https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5}
}

@misc{chen2020quantumlongshorttermmemory,
      title={Quantum Long Short-Term Memory},
      author={Samuel Yen-Chi Chen and Shinjae Yoo and Yao-Lung L. Fang},
      year={2020},
      eprint={2009.01783},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2009.01783}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A. and Schmidhuber, J{"u}rgen and Cummins, Fred},
  journal={Neural Computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press},
  doi={10.1162/089976600300015015}
}

@article{greff2017lstm,
  author={Greff, Klaus and Srivastava, Rupesh K. and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  title={LSTM: A Search Space Odyssey},
  year={2017},
  volume={28},
  number={10},
  pages={2222-2232},
  doi={10.1109/TNNLS.2016.2582924}
}

@inproceedings{krueger2017zoneout,
  title={Zoneout: Regularizing rnns by randomly preserving hidden activations},
  author={Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{semeniuta2016recurrent,
  title={Recurrent dropout without memory loss},
  author={Semeneniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={173--182},
  organization={IEEE}
}

@inproceedings{jozefowicz2015empirical,
  title = {An Empirical Exploration of Recurrent Network Architectures},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  pages = {2342--2350},
  year = {2015},
  editor = {Bach, Francis and Blei, David},
  volume = {37},
  series = {JMLR Workshop and Conference Proceedings},
  address = {Lille, France},
  month = {07--09 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v37/jozefowicz15.pdf},
  url = {http://proceedings.mlr.press/v37/jozefowicz15.html}
}

@inproceedings{he2016deep,
  title={Deep Residual Learning for Image Recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  month={Jun},
  publisher={{IEEE}},
  doi={10.1109/CVPR.2016.90}
}

@Article{s21206758,
AUTHOR = {Wang, Xiujuan and Sui, Yi and Zheng, Kangfeng and Shi, Yutong and Cao, Siwei},
TITLE = {Personality Classification of Social Users Based on Feature Fusion},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {6758},
URL = {https://www.mdpi.com/1424-8220/21/20/6758},
PubMedID = {34695969},
ISSN = {1424-8220},
ABSTRACT = {Based on the openness and accessibility of user data, personality recognition is widely used in personalized recommendation, intelligent medicine, natural language processing, and so on. Existing approaches usually adopt a single deep learning mechanism to extract personality information from user data, which leads to semantic loss to some extent. In addition, researchers encode scattered user posts in a sequential or hierarchical manner, ignoring the connection between posts and the unequal value of different posts to classification tasks. We propose a hierarchical hybrid model based on a self-attention mechanism, namely HMAttn-ECBiL, to fully excavate deep semantic information horizontally and vertically. Multiple modules composed of convolutional neural network and bi-directional long short-term memory encode different types of personality representations in a hierarchical and partitioned manner, which pays attention to the contribution of different words in posts and different posts to personality information and captures the dependencies between scattered posts. Moreover, the addition of a word embedding module effectively makes up for the original semantics filtered by a deep neural network. We verified the hybrid model on the MyPersonality dataset. The experimental results showed that the classification performance of the hybrid model exceeds the different model architectures and baseline models, and the average accuracy reached 72.01%.},
DOI = {10.3390/s21206758}
}

% [33] Mixed precision training
@inproceedings{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  booktitle={International conference on learning representations},
  year={2018},
  url={https://arxiv.org/abs/1710.03740}
}